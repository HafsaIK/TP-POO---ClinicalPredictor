import numpy as np
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                            f1_score, confusion_matrix, classification_report)

class ModelEvaluator:
    """Classe pour Ã©valuer les modÃ¨les avec mÃ©triques intÃ©grÃ©es"""
    
    def __init__(self, model):
        self.model = model
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> dict:
        """
        Calcule toutes les mÃ©triques principales
        
        Args:
            y_true: Vraies valeurs
            y_pred: Valeurs prÃ©dites
            
        Returns:
            Dictionnaire avec toutes les mÃ©triques
        """
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),
            'f1_score': f1_score(y_true, y_pred, average='binary', zero_division=0)
        }
        return metrics
    
    def print_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray):
        """
        Affiche la matrice de confusion de maniÃ¨re formatÃ©e
        
        Args:
            y_true: Vraies valeurs
            y_pred: Valeurs prÃ©dites
        """
        cm = confusion_matrix(y_true, y_pred)
        print("\nðŸ“Š Matrice de Confusion:")
        print("    PrÃ©dit Sain | PrÃ©dit Malade")
        print(f"RÃ©el Sain:     {cm[0][0]:3d}    |    {cm[0][1]:3d}")
        print(f"RÃ©el Malade:   {cm[1][0]:3d}    |    {cm[1][1]:3d}")
        
    def print_classification_report(self, y_true: np.ndarray, y_pred: np.ndarray):
        """
        Affiche le rapport de classification complet
        
        Args:
            y_true: Vraies valeurs
            y_pred: Valeurs prÃ©dites
        """
        print("\nðŸ“‹ Rapport de Classification:")
        print(classification_report(y_true, y_pred, 
                                   target_names=['Sain', 'DiabÃ©tique'],
                                   zero_division=0))
        
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> dict:
        """
        Ã‰value le modÃ¨le sur les donnÃ©es de test et affiche tous les rÃ©sultats
        
        Args:
            X_test: Features de test
            y_test: Target de test
            
        Returns:
            Dictionnaire avec toutes les mÃ©triques
        """
        print("\n" + "="*60)
        print("ðŸ“ˆ Ã‰VALUATION DU MODÃˆLE")
        print("="*60)
        
        y_pred = self.model.predict(X_test)
        metrics = self.calculate_metrics(y_test, y_pred)
        
        print(f"\nðŸŽ¯ MÃ©triques de Performance:")
        print(f"   Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall:    {metrics['recall']:.4f}")
        print(f"   F1-Score:  {metrics['f1_score']:.4f}")
        
        self.print_confusion_matrix(y_test, y_pred)
        self.print_classification_report(y_test, y_pred)
        
        return metrics